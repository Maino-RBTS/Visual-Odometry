# Structure From Motion ğŸ‘ˆ

**REFERENCE : "Structure-from-motion Revisited" in CVPR (2016)**

SFM is extracts 3D information from 2D image. SFM goes through several processes

![image](https://user-images.githubusercontent.com/60316325/231698586-5a318bd1-7b5e-4027-82f1-cc4cc91c99ae.png)

1. **Camera Calibration ( Intrinsic Parameters & Extrinsic Parameters )**
2. **Feature Extraction**
3. **Description = Feature Matching**
4. **Epipolar Geometry**
5. **Triangulation**
6. **Bundle Adjustment**

---

### 1. Camera Calibration ğŸ“·

The real world we see with our eyes is 3D. However, when taken with a camera, it becomes a 2D image.

Where the 3D points are projected on the image is determined by the position and direction of the camera.

Further, the Actual image is affected by the mechanical parts inside the camera used. <br>(Lens, distance from lens, angle etc..)

When restoring 3D space coordinates from the positions of 3D points projected in image or from points on 2D, <br>
intrinsic parmeter must be known.

Thus, The process of finding **Intrinsic Parameter** is called "Camera Calibration". <br>
(í•œë§ˆë””ë¡œ, ì¹´ë©”ë¼ ë‚´ë¶€ íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ì•„ë‚´ëŠ” ê³¼ì •ì„ ë§í•œë‹¤.)

Perspective Projection of 3D point to 2D image plane

![image](https://user-images.githubusercontent.com/60316325/231688055-4da488e9-cd67-4bf4-9df5-1059310f5dac.png)

![image](https://user-images.githubusercontent.com/60316325/231683702-a868464b-a6dc-4824-9c25-370b827bc5bd.png)

![image](https://user-images.githubusercontent.com/60316325/231683990-4587b6f6-574f-486a-808c-cd2ce958f60a.png)

* $X, Y, Z$ = 3D Position respect to world coordinate system <br>
* $[R|t]$ = Rotation Matrix to transform world coordinate respect to camera coordinate frame (extrinsic parameter), R=rotation, t=moved
* $A$ = Intrinsic camera matrix
* $A[R|t]$ = Camera matrix or Projection Matrix

* $f_x, f_y$ = Focal length : The distance between the lens and the point on the image plane.
* $c_x, c_y$ = Principal point : Center of camera lens.
* $skew$ _ $cf_x$ = assymmetry coefficient (ë¹„ëŒ€ì¹­ ê³„ìˆ˜) : The degree of tilt of the y-axis of the image sensor.

In general, Camera Calibration is proceed by using check pattern board below...

![image](https://user-images.githubusercontent.com/60316325/231694191-81ea44ad-8678-4d19-8b71-e7e6f7f66ba8.png)

---

### 2. Feature Extraction âœ¨

This is the process of extracting feature points such as corners or edges. <br>
However, ther is a problem that is vulnerable to movement, rotation and scale conversion of the image.

To solve this problem, various techniques such as **SIFT, SURF, BRISK, ORB, FAST** have been used and developed.

---

### 3. Feature Matching âœ…

![image](https://user-images.githubusercontent.com/60316325/231700894-c9f3d68c-b19e-481e-904a-fd575298f738.png)

This is a step of matching points having the same characteristics between images based on the feature points extracted from each images.<br>
No geometrical meaning is included...

---

### 4. Epipolar Geometry ğŸ“· ğŸ“·

Based on the images from two different view, It is possible to find out the geometric relationship between the points matched in the previous step.

![image](https://user-images.githubusercontent.com/60316325/231701711-95d90f64-bc79-4ad6-b8d2-376e54d6fdcb.png)

For easy explanation, called image plane of camera 1 = A, camera 2 = B

* $x$ = Point in 3D space.
* $\bar{x}\_1, \bar{x}\_2$ = Points in 3D space projected on a 2D image (2D ì´ë¯¸ì§€ í‰ë©´ìƒì— íˆ¬ì˜ëœ 3ì°¨ì› ê³µê°„ìƒì˜ ì )
* $e_1, e_2$ = Epipole : The point at which center of a camera at another viewpoint is projected onto the image plane of a specific camera. <br>
(ë‹¤ë¥¸ ì‹œì ì˜ ì¹´ë©”ë¼ ì¤‘ì‹¬ì ($c_2$)ì„, íŠ¹ì • ì¹´ë©”ë¼($c_1$)ì˜ ì´ë¯¸ì§€ í‰ë©´ì— íˆ¬ì˜ì‹œì¼°ì„ ë•Œì˜ ì )
* $\tilde{l}\_1, \tilde{l}\_2$ = Epipolar line : The line connecting $\bar{x}$ and $e$.
* Baseline = The distance between two cameras.
* $[R|t]$ = A matrix representing how much $c_2$ is rotated and moved relative to $c_1$. <br>
(c1 ì¹´ë©”ë¼ë¥¼ ê¸°ì¤€ìœ¼ë¡œ c2 ì¹´ë©”ë¼ê°€ ì–´ë–¤ íšŒì „ê³¼ ì´ë™ì´ ì´ë£¨ì–´ì¡ŒëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” í–‰ë ¬)

Even if we know $[R|t]$, if we don't know depth(distance) about point $x$, we can't know accurate point of $x$.

If straight line that connecting $c_1$ and $x$ is projected onto $c_2$ image plane, It becomes "Epipolar Line ($\tilde{I}\_2$)" <br>
That is, if we know the positional relationship between the image and the camera for the two views, we cannot match points to points, <br>
but we can match points to lines. <br>
(ì¦‰ 2ê°œì˜ ì„œë¡œë‹¤ë¥¸ ë·°ì— ëŒ€í•œ ì´ë¯¸ì§€ì™€ ì¹´ë©”ë¼ê°„ì˜ ìœ„ì¹˜ê´€ê³„($[R|t]$)ë¥¼ ì•ˆë‹¤ë©´, ì ê³¼ ì ì„ ëŒ€ì‘ì‹œí‚¬ ìˆ˜ëŠ” ì—†ì§€ë§Œ ì ê³¼ ì„ ì„ ëŒ€ì‘ì‹œí‚¬ ìˆ˜ëŠ” ìˆë‹¤.)

ì‰½ê²Œ, A ìƒì˜ ì˜ìƒì¢Œí‘œ $\bar{x}\_1$ë¥¼ ì•Œê³  ìˆì„ ë•Œ, ì´ë¥¼ Bì—ì„œ ëŒ€ì‘ë˜ëŠ” ì˜ìƒ ì¢Œí‘œ $\bar{x}\_2$ì˜ ì¢Œí‘œë¥¼ êµ¬í•  ë•Œ <br>
3ì°¨ì› ìƒì˜ ì  $x$ê¹Œì§€ì˜ ê±°ë¦¬(depth)ì •ë³´ë¥¼ ëª¨ë¥¸ë‹¤ë©´, $x$ ì˜ ì¢Œí‘œë¥¼ ë³µì›í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ ì´ë¥¼ Bì— íˆ¬ì˜ì‹œí‚¨ $\bar{x}\_2$ì˜ ì¢Œí‘œ ë˜í•œ ê²°ì •í•  ìˆ˜ ì—†ë‹¤. <br>
í•˜ì§€ë§Œ ì  $x$ëŠ” $c_1$ ê³¼ $\bar{x}\_1$ë¥¼ ì‡ëŠ” ì§ì„ ìƒì— ì¡´ì¬í•˜ë¯€ë¡œ, ì´ ì§ì„ ì„ Bì— íˆ¬ì˜ì‹œí‚¤ë©´ ì  $\bar{x}\_2$ê°€ íˆ¬ì˜ëœ ì§ì„ ì— ìˆìŒì„ ì•Œ ìˆ˜ ìˆë‹¤.  

So. The matrix representing the correspondence between such points and lines is called **"Essential Matrix"** and **"Fundamental Matrix"**

#### 4.1 Essential Matrix

E MatrixëŠ” ì •ê·œí™”ëœ ì´ë¯¸ì§€ í‰ë©´ì—ì„œ ë§¤ì¹­ ìŒ($\bar{x}\_1$, $\bar{x}\_2$)ë“¤ ì‚¬ì´ì˜ ê¸°í•˜í•™ì  ê´€ê³„ë¥¼ ì„¤ëª…í•˜ëŠ” í–‰ë ¬<br>
Eë¥¼ ì•Œë©´, ë‘ ì¹´ë©”ë¼ ì‹œì  ì‚¬ì´ì˜ íšŒì „, í‰í–‰ì´ë™ ê´€ê³„ë¥¼ íŒŒì•… ê°€ëŠ¥í•˜ë‹¤.

Before learning about Essential Matrix, understand concepts below. 

* **Homogeneous** í•˜ë‹¤ëŠ” ì˜ë¯¸ëŠ”, ì´ë¯¸ì§€ í‰ë©´ìƒì˜ ì¢Œí‘œ ($x$, $y$)ë¥¼, ($x$, $y$, $1$)ë¡œ í‘œí˜„í•œë‹¤ëŠ” ê²ƒ, 3ì°¨ì›ì˜ ê²½ìš° ($x$, $y$, $z$, $1$)<br>
ì´ë ‡ê²Œ ë˜ë©´, affine ë³€í™˜ì´ë‚˜ perspective(projective) ë³€í™˜ì„ í•˜ë‚˜ì˜ ë‹¨ì¼ í–‰ë ¬ë¡œ í‘œí˜„í•  ìˆ˜ ìˆë‹¤ëŠ” ì¥ì ì´ ìˆë‹¤. <br>
[Homogeneous means that coordinates ($x$, $y$) on the image plane are expressed as ($x$, $y$, $1$), in the case of 3D, ($x$, $y$, $z$, $1$) <br>
This has the advantage of being able to express affine transformations or perspective(projective) transformations in a single matrix.]

* **Nomarlized image plane** ì€ ì •ê·œì¢Œí‘œê³„ ë¼ê³ ë„ í•˜ë©° 'ì¹´ë©”ë¼ì˜ ë‚´ë¶€ íŒŒë¼ë¯¸í„°(intrinsic)ì˜ ì˜í–¥ì„ ì œê±°í•œ' ì´ë¯¸ì§€ ì¢Œí‘œê³„ì´ë‹¤. <br>
ì¦‰, ì •ê·œí™”ë˜ì—ˆìœ¼ë¯€ë¡œ ì¹´ë©”ë¼ ì´ˆì ê³¼ì˜ ê±°ë¦¬ê°€ 1ì¸ ê°€ìƒì˜ ì´ë¯¸ì§€ í‰ë©´ì„ ì •ì˜í•œë‹¤. <br> (ì›ë˜ì˜ ì´ë¯¸ì§€ í‰ë©´ì„ ì´ˆì ê³¼ì˜ ê±°ë¦¬ê°€ 1ì¸ ì§€ì ìœ¼ë¡œ ì˜®ê²¨ë†“ì•˜ë‹¤ê³  ìƒê°) <br>
ì„œë¡œ ë‹¤ë¥¸ ì¹´ë©”ë¼ë¡œ ì°ë”ë¼ë„, Intrinsicì„ ì œê±°í–ˆìœ¼ë¯€ë¡œ ì¼ê´€ëœ ê¸°í•˜í•™ì ì¸ í•´ì„ì„ í•˜ëŠ”ë° íš¨ê³¼ì ì´ê¸°ì— ë„ì…ë˜ì—ˆìŒ. <br>
[Normalized image plane is called Normalized coordinate system that removed the effect of the camera's intrinsic parameters <br>
That is, since it is normalized, a virtual image plane with a distance of 1 from the camera focal point is defined. <br>
(Think of moving the original image plane to a point where the distance from focal point is 1) <br>
Even if it is taken with different cameras, it is effective for cosistent geometric analysis because Intrinsic has been removed]

If $x$ projected to $\bar{x}\_1$ at A and $\bar{x}\_2$ at B <br>
Essential Matrix describing the relationship between $\bar{x}\_1$ and $\bar{x}\_2$. <br>
(But, $\bar{x}\_1$ and $\bar{x}\_2$ are **homogeneous coordinate on normalized image plane**)

$$ x'^{\ T} \mathbf{E} x = 0 \quad ... (1)$$

$$ 
\begin{bmatrix}
u' &  v' & 1
\end{bmatrix}
\mathbf{E}
\begin{bmatrix}
u \\
v \\
1
\end{bmatrix} = 0 \quad ... (2)
$$

* $u, v$ = Point $\bar{x}\_1$ on image plane A, $u', v'$ = Point $\bar{x}\_2$ on image plane B. (coordinate)

Equation (1) called **"Epipolar Constraints"** and E matrix called **"Essential Matrix"**.

$$ \mathbf{E} = \mathbf{R} \ [t]\_{\times} $$

* $R$ = Rotation Matrix, $t$ = Parallel Movement vector.

#### 4.2 Fundamental Matrix

F MatrixëŠ” ì¹´ë©”ë¼ íŒŒë¼ë¯¸í„°ê¹Œì§€ í¬í•¨ëœ ë‘ ì´ë¯¸ì§€ì˜ ì‹¤ì œ í”½ì…€ ì¢Œí‘œ ì‚¬ì´ì˜ ê¸°í•˜í•™ì  ê´€ê³„ë¥¼ í‘œí˜„í•˜ëŠ” í–‰ë ¬

For Any two image plane(A,B), A matrix exists that satisfying relationship between $\bar{x}\_1$, $\bar{x}\_2$. <br>
F Matrix that satisfies this relationship is called **"Fundamental Matrix"**

$$ \bar{x}\_2^{\ T} \mathbf{F} \bar{x}\_1 = 0 \quad $$

$$ 
\begin{bmatrix}
x' &  y' & 1
\end{bmatrix}
\mathbf{F}
\begin{bmatrix}
x \\
y \\
1
\end{bmatrix} = 0
$$

---

### 5. Triangulation ğŸ“

ì‚¼ê°ì¸¡ëŸ‰ë²• ì´ë¼ê³ ë„ í•œë‹¤.

If geometric relationship between two image planes ( Essential and Fundamental Matrix) is given and <br>
matching pair $\bar{x}\_1$, $\bar{x}\_2$ on the two image planes given, we can determine the original 3D coordinate $x$ from them

![image](https://user-images.githubusercontent.com/60316325/231755896-0109c505-31c9-4657-8a74-179f5bb83c65.png)

* $c_1, c_2$ : Principal Points
* $p_1, p_2$ : Projected 2D Feature Point From 3D point in each image plane

Two straight line \overrightarrow{$c_1p_1$} and  \overrightarrow{$c_2p_2$} is intersect at P <br>
(But, Actually it may not completely intersect due to noise like above image)

To solve this, there are many method pesented. In Wikipedia, there are three typical method <br>
( Refer this ! : https://en.wikipedia.org/wiki/Triangulation_(computer_vision) )

More detail about "Triangulation" will be explained in its part (Triangulation directory)

---

### 6. Bundle Adjustment ğŸ“‚

![image](https://user-images.githubusercontent.com/60316325/231939338-7460eec9-fad4-406f-9aef-5216a9e68755.png)

![image](https://user-images.githubusercontent.com/60316325/231939360-4e8dad2d-0d1f-44a8-98e1-cb5b56303ca0.png)

Bundle Adjustment is to optimize the relative 3D motion between the camera frame and the 3D point loaction that can be estimated <br>
based on the locations of keypoints that exist in multiple frames.

From Triangulation, we can get 3D coordinate position. $P_1^'$, $P_2^'$, $P_3^'$ are coordinate position that reprojected 3D point <br>
(after Triangulation) to 2D image plane. so this is not the original coordinate.

The overall flow is as below..

1. Input image and Extract(Detect) Feature Points.
2. From the Feature Points, we can estimate Camera pose. ($R$ matrix and $t$ vector)
3. Determine 3D point position using Triangulation.
4. Correct Camera pose and 3D Point using Bundle Adjustment, Output = Corrected Camera pose and 3D Point position

Usually, use Outlier Filtering method (e.g RANSAC) and Bundle Adjustment together to reduce reprojection error.

---
